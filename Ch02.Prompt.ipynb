{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ad5951",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  \"Ch02.Prompt\"\n",
    "date:   2015-06-26 02:00:00 +0700\n",
    "categories: [LLM]\n",
    "---\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}});\n",
    "</script>\n",
    "<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "\n",
    "### 참조 사이트.\n",
    "해당 Post는 <a href=\"https://wikidocs.net/book/14314\">LangChain 위키독스</a>에 나와있는 예제와 흐름을 파악하는 용도 입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ce5a82",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "\n",
    "LLM의 Input으로 들어가게 되는 Prompt에 대해 자세히 알아본다.\n",
    "\n",
    "Prompt란 아래와 같이 정의하고 있다.\n",
    "\n",
    ">프롬프트 단계는 검색기에서 검색된 문서들을 바탕으로 **언어 모델이 사용할 질문이나 명령을 생성하는 과정**입니다. 이 단계는 **검색된 정보를 바탕**으로 최종 사용자의 질문에 가장 잘 대응할 수 있는 응답을 생성하기 위해 필수적인 단계입니다.\n",
    "\n",
    "Prompt는 아래와 같은 필요성을 가지고 있다.\n",
    "- 문맥(Context) 설정: LLM에 추가적인 정보를 전달함으로서 할루시네이션을 줄이는 역할로 사용 가능하다.\n",
    "- 정보 통합: 서로 다른 여러 정보를 하나로 합쳐 정보를 전달 할 수 있다.\n",
    "- 응답 품질 향상: RAG에서 많이 사용되는 방법으로서, 도움이 되는 Context를 선택하여 전달 할 수 있다.\n",
    "\n",
    "기본적인 RAG에서 많이 사용되는 구조는 아래와 같다.\n",
    "- Instruction: 지시사항으로서, LLM에 어떤 역할로서 답변을 얻어낼지 지정한다.\n",
    "- Question: 사용자의 질문을 전달한다.\n",
    "- Context: RAG로서 검색된 정보를 전달한다. 답변생성에 있어서 할루시네이션을 줄이는 역할을 한다.\n",
    "\n",
    "기본적인 RAG의 Prompt를 확인하면 아래와 같다.\n",
    "\n",
    "```python\n",
    "당신은 질문-답변(Question-Answer) Task 를 수행한는 AI 어시스턴트 입니다.\n",
    "검색된 문맥(context)를 사용하여 질문(question)에 답하세요. \n",
    "만약, 문맥(context) 으로부터 답을 찾을 수 없다면 '모른다' 고 말하세요. \n",
    "한국어로 대답하세요.\n",
    "\n",
    "#Question: \n",
    "{이곳에 사용자가 입력한 질문이 삽입됩니다}\n",
    "\n",
    "#Context: \n",
    "{이곳에 검색된 정보가 삽입됩니다}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f218e74",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182e24a",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cf379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, load_prompt\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c28aba",
   "metadata": {},
   "source": [
    "#### Model 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c54d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    model=\"Qwen/Qwen3-4B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcde33d",
   "metadata": {},
   "source": [
    "#### 사용할 Prompt 저장.\n",
    "\n",
    "사용할 Prompt를 미리 지정하여 저장한다.\n",
    "\n",
    "Appendix: OpenAI에서 제공하는 default prompt가 있으나, 실제 해당 값과 동일하게 저장하고 불러와서 확인하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150effc",
   "metadata": {},
   "source": [
    "fruit_color.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e774a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = {\n",
    "    \"_type\": \"prompt\",\n",
    "    \"input_variables\": [\"fruit\"],\n",
    "    \"template\": \"What color is a {fruit}?\"\n",
    "}\n",
    "\n",
    "Path(\"prompts\").mkdir(exist_ok=True)\n",
    "with open(\"./data/fruit_color.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(prompt_data, f, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319ce65",
   "metadata": {},
   "source": [
    "load_prompt로 불러와서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93adc653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['fruit'] input_types={} partial_variables={} template='What color is a {fruit}?'\n"
     ]
    }
   ],
   "source": [
    "prompt = load_prompt(\"./data/fruit_color.yaml\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff9497",
   "metadata": {},
   "source": [
    "실제 load_prompt를 확인하게 되면, 아래와 같이 type이 **langchain_core.prompts.prompt.PromptTemplate**인 것을 확인할 수 있다.  \n",
    "위와 같이 바뀌는 이유는 LangChain인 내부에서 _type을 확인하고 자동으로 Mapping하여 가져오기 때문이다.\n",
    "\n",
    "```python\n",
    "if _type == \"prompt\":\n",
    "    return PromptTemplate(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d536fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.prompt.PromptTemplate"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6fab6",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "\n",
    "#### 방법 1. from_template() 메소드를 사용하여 PrompteTemplate 객체를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10858061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['country'] input_types={} partial_variables={} template='{country}의 수도는 어디인가요?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 **서울**입니다.  \\n서울은 한국의 정치, 경제, 문화 중심지로, 국가의 주요 행정 기관과 국제적인 중심 도시로 알려져 있습니다.  \\n역사적으로 서울은 한국의 중심 도시로 자리 잡고 있으며, 일본의 식민지 시대에는 수도로 사용되었고, 1945년 해방 후 다시 서울이 수도로 정해졌습니다.  \\n다른 주요 도시(부산, 인천 등)는 서울과는 구분되는 대도시이지만, 수도는 서울입니다.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의. {country}는 변수로, 이후에 값이 들어갈 자리를 의미\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(type(prompt))\n",
    "print(prompt)\n",
    "\n",
    "# chain 생성\n",
    "chain = prompt | model\n",
    "\n",
    "# country 변수에 입력된 값이 자동으로 치환되어 수행됨\n",
    "chain.invoke(\"대한민국\").content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d1e9a",
   "metadata": {},
   "source": [
    "#### 방법 2. PromptTemplate 객체 생성과 동시에 prompt 생성\n",
    "\n",
    "input_variables를 사용하여, PromptTemplate를 생성한다.\n",
    "\n",
    "**appendix: number of input_variables**: input_variables가 1개인 경우에는 invoke의 값이 자동으로 귿어가게 된다.  \n",
    "하지만, 2개 이상인 경우에는 dictionary 형태로 전달해야 LLM에 Input으로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c2876",
   "metadata": {},
   "source": [
    "input_variables가 1개인 경우."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1972a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['country'] input_types={} partial_variables={} template='{country}의 수도는 어디인가요?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 **서울**입니다.  \\n서울은 한국의 정치, 경제, 문화 중심지로, 국가의 주요 기관(국회, 대통령 집안, 행정부 등)이 위치해 있습니다.  \\n또한 서울은 역사적, 문화적 유산이 풍부한 도시로, 세계적인 관광지로도 유명합니다.  \\n다른 대도시인 부산, 인천 등과는 달리, 서울은 대한민국의 정부 기관이 집중된 수도 도시입니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# PromptTemplate 객체를 활용하여 prompt_template 생성\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)\n",
    "\n",
    "# chain 생성\n",
    "chain = prompt | model\n",
    "\n",
    "# country 변수에 입력된 값이 자동으로 치환되어 수행됨\n",
    "chain.invoke(\"대한민국\").content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc4c2f",
   "metadata": {},
   "source": [
    "input_variables가 2개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5d092b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected mapping type as input to PromptTemplate. Received <class 'str'>.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m chain = prompt | model\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# country 변수에 입력된 값이 자동으로 치환되어 수행됨\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m대한민국\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.content.split(\u001b[33m'\u001b[39m\u001b[33m</think>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\LLM\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\LLM\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\LLM\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\LLM\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\LLM\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     _inner_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**_inner_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\LLM\\Lib\\site-packages\\langchain_core\\prompts\\base.py:165\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    161\u001b[39m         msg = (\n\u001b[32m    162\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected mapping type as input to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inner_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    166\u001b[39m             create_message(\n\u001b[32m    167\u001b[39m                 message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT\n\u001b[32m    168\u001b[39m             )\n\u001b[32m    169\u001b[39m         )\n\u001b[32m    170\u001b[39m missing = \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.input_variables).difference(inner_input)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing:\n",
      "\u001b[31mTypeError\u001b[39m: Expected mapping type as input to PromptTemplate. Received <class 'str'>.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT "
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"{country1}과 {country2}의 수도는 각각 어디인가요?\",\n",
    "    input_variables=[\"country1\", \"country2\"],\n",
    ")\n",
    "\n",
    "# chain 생성\n",
    "chain = prompt | model\n",
    "\n",
    "# country 변수에 입력된 값이 자동으로 치환되어 수행됨\n",
    "chain.invoke(\"대한민국\").content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3794300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 **서울**이고, 일본의 수도는 **东京(도쿄)**입니다.  \\n\\n- **서울(Seoul)**: 대한민국의 정치, 경제, 문화 중심지로, 세계적인 도시로 유명합니다. 역사적인 장소인 경복궁과 현대적인 빌딩들이 어우러진 도시입니다.  \\n- **도쿄(Tokyo)**: 일본의 정치, 경제, 문화 중심지로, 세계적인 재무 중심지로 자리잡고 있습니다. 역사적 유적지인 일본 왕실 궁전과 현대적인 도시 풍경이 조화롭게 어우러집니다.  \\n\\n두 도시 모두 각국의 중심지로서 중요한 역할을 합니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country1\": \"대한민국\", \"country2\": \"일본\"}).content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195087",
   "metadata": {},
   "source": [
    "partital_variables를 사용하여, dictionary형태로 input value와 default value를 적용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141e3ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['country1'] input_types={} partial_variables={'country2': '미국'} template='{country1}과 {country2}의 수도는 각각 어디인가요?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 **서울**입니다.  \\n미국의 수도는 **워싱턴 D.C.**(Washington, D.C.)입니다.  \\n\\n### 추가 정보:\\n- **서울**은 대한민국의 정치, 경제, 문화 중심지로, 1948년 이후로 수도로 지정되어 있습니다.  \\n- **워싱턴 D.C.**는 미국의 정부 기관(미국 대통령 주거지인 백악관, 상원 및 하원, Supreme Court 등)이 위치한 연방 구역으로, 주요 도시가 아닌 공식적인 수도입니다.  \\n\\n이 두 도시는 각국의 정치적 중심지로 중요한 역할을 합니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의\n",
    "template = \"{country1}과 {country2}의 수도는 각각 어디인가요?\"\n",
    "\n",
    "# PromptTemplate 객체를 활용하여 prompt_template 생성\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country1\"],\n",
    "    partial_variables={\n",
    "        \"country2\": \"미국\"  # dictionary 형태로 partial_variables를 전달\n",
    "    },\n",
    ")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)\n",
    "\n",
    "# chain 생성\n",
    "chain = prompt | model\n",
    "\n",
    "# country1 변수에 입력된 값이 자동으로 치환되어 수행됨 (input_variables가 1개 이므로.)\n",
    "chain.invoke(\"대한민국\").content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49e830",
   "metadata": {},
   "source": [
    "**partitial variables**: partital_varirables에는 함수(Callable)의 값을 넣을 수 있지만, input_variables에는 함수는 안됩니다. 반드시 입력값으로 직접 값을 전달하여야 합니다.\n",
    "\n",
    "partital variables에 함수값을 받는 대표적인 예시는 날짜를 반환하는 함수로서 사용법은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707fdc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'June 28일에 생일을 맞이한 유명인 3명은 다음과 같습니다. 각자의 생년월일을 기재하였습니다:\\n\\n1. **Sir Peter Ustinov (1928년 6월 28일 - 2004년 9월 28일)**  \\n   - 영국의 배우, 풍자극작가, 정치인. \"The Ladykillers\" 등의 영화에서 유명한 인물로, 1980년대에 BBC에서 라디오 연기자로 활동했으며, 1990년대에는 정치적 라디오 프로그램을 진행했습니다.\\n\\n2. **Terry Pratchett (1948년 6월 28일 - 2015년 6월 12일)**  \\n   - 영국의 소설가로, \"Discworld\" 시리즈를 기반으로 한 판타지 소설을 쓴 인물. \"The Color of Magic\"와 \"Night Watch\" 등이 유명합니다. 1990년대에 \"The Hitchhiker\\'s Guide to the Galaxy\" 시리즈를 집필했으며, 2005년에 \"Good Omens\"을 공동 집필했습니다.\\n\\n3. **Lance Armstrong (1971년 6월 28일 - 현재)**  \\n   - 미국의 자전거 선수로, 7회 토리에 챔피언십 우승자로 유명합니다. 1999년부터 2005년까지 7회 우승을 차지했지만, 2013년에 음주 및 약물 사용 혐의로 2005년 우승을 무효화당했습니다. 자전서 \"The Tour\"를 저술했습니다.\\n\\n이 외에도 일부 인물은 생일이 June 28일이지만, 그들의 유명도나 영향력이 상대적으로 낮은 경우가 있습니다. 위의 목록은 주로 인류 역사상 큰 영향을 미친 인물들을 중심으로 구성했습니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 날짜를 반환하는 함수 정의\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%B %d\")\n",
    "\n",
    "# prompt 정의\n",
    "prompt = PromptTemplate(\n",
    "    template=\"오늘의 날짜는 {today} 입니다. 오늘이 생일인 유명인 {n}명을 나열해 주세요. 생년월일을 표기해주세요.\",\n",
    "    input_variables=[\"n\"],\n",
    "    partial_variables={\n",
    "        \"today\": get_today  # 함수의 return값으로서 partial_variables값 전달.\n",
    "    },\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "chain = prompt | model\n",
    "chain.invoke(3).content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914cf5b",
   "metadata": {},
   "source": [
    "함수값으로서 선언하여도, 아래와 같이 String으로 다시 값을 치환하여서 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45390fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지난 1월 2일에 태어난 유명인 3명을 아래에 소개합니다. 생년월일은 정확한 정보를 기반으로 제공되었으나, 일부 인물의 출생일이 오류가 있을 수 있으니 참고하시기 바랍니다.\\n\\n1. **스티븐 스皮尔버그 (Steven Spielberg)**  \\n   - **생년월일**: 1946년 1월 2일  \\n   - **소속 분야**: 영화 감독, 제작자  \\n   - **특징**: 세계적으로 유명한 영화 감독으로 《인셉션》, 《조지아의 여름》 등 다양한 작품을 제작한 바 있다.\\n\\n2. **존 레전드 (John Legend)**  \\n   - **생년월일**: 1987년 1월 2일  \\n   - **소속 분야**: 가수, 작곡가  \\n   - **특징**: 블랙 프라이데이 쇼를 비롯한 다양한 공연에서 활동하며, 음악적 재능을 인정받은 인물.\\n\\n3. **조지 루카스 (George Lucas)**  \\n   - **생년월일**: 1944년 1월 2일  \\n   - **소속 분야**: 영화 감독, 제작자, 시나리오작가  \\n   - **특징**: 《스타워즈》 시리즈의 창작자로 영화 산업에 큰 영향을 미친 인물.\\n\\n**참고**: 일부 인물의 출생일이 오류가 있을 수 있으므로, 정확한 정보는 공식적인 출처를 확인하시기 바랍니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"today\": \"Jan 02\", \"n\": 3}).content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0efc55",
   "metadata": {},
   "source": [
    "#### 방법 3. File로서 값을 가져오는 방법.\n",
    "\n",
    "LangChain에서 지원하는 PromptTemplate로서 file을 읽고 만드는 법은, 아래와 같이 load_prompt를 사용하는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cb8de95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A 사과 (apple) can come in various colors depending on the variety. The most common colors are **red**, **green**, and **yellow**, but other colors like **purple** or **orange** are also possible. For example:  \\n- **Red apples** (e.g., Red Delicious)  \\n- **Green apples** (e.g., Granny Smith)  \\n- **Yellow apples** (e.g., Honeycrisp)  \\n\\nThe color is determined by the apple's species and growing conditions. So, while red is the most typical answer, apples can be many different colors! 🍎\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = load_prompt('./data/fruit_color.yaml')\n",
    "chain = prompt | model\n",
    "chain.invoke(\"사과\").content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b4d1d",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "\n",
    "ChatPromptTemplate는 대화목록을 프롬프트로 주입하고자 할 때 활용할 수 있습니다.\n",
    "\n",
    "```python\n",
    "ChatPromptTemplate.from_messages(\n",
    "    messages: List[Union[Tuple[str, str], BaseMessagePromptTemplate]],\n",
    "    input_variables: Optional[List[str]] = None,\n",
    "    partial_variables: Optional[Dict[str, Any]] = None\n",
    ")\n",
    "```\n",
    "\n",
    "- <code>messages</code>: 메시지들을 순서대로 나열한 리스트. 각 메시지는 (\"role\", \"template string\") 형태의 튜플이거나 SystemMessagePromptTemplate, HumanMessagePromptTemplate 등의 객체로 구성할 수 있음.\n",
    "- <code>input_variables</code>: {}로 들어가는 변수명들의 리스트 (자동 추론되므로 대부분 생략 가능)\n",
    "- <code>partial_variables</code>: 일부 변수를 미리 값 또는 함수로 지정 (예: 날짜, 사용자 ID 등 고정값 처리용)\n",
    "\n",
    "**즉 ChatPromptTemplate란, 이전 대화들의 주체를 Role로서 구성하고 나눈 대화들을 content로서 넣어서 // 자연스러운 대화 결과를 얻기 위한 Prompt라고 생각할 수 있다.**\n",
    "\n",
    "\n",
    "#### PromptTemplate vs. ChatPromptTemplate\n",
    "\n",
    "| 항목          | `PromptTemplate`                              | `ChatPromptTemplate`                                     |\n",
    "|---------------|-----------------------------------------------|-----------------------------------------------------------|\n",
    "| **형식**       | 일반 텍스트 (문장 하나)                        | Chat 형태 (역할 기반 메시지 나열)                          |\n",
    "| **역할 설정**  | ❌ 없음                                       | ✅ `system`, `user`, `assistant` 등의 역할 지정 가능         |\n",
    "| **사용 모델**  | 일반 LLM (`text-davinci`, `llama`, 등)        | Chat LLM (`gpt-3.5-turbo`, `gpt-4`, `claude` 등)           |\n",
    "| **메시지 순서**| 하나의 문자열                                 | 여러 메시지를 순서대로 입력 가능                            |\n",
    "| **구성 요소**  | `{}`로 변수를 치환하는 하나의 문자열            | `SystemMessagePromptTemplate`, `HumanMessagePromptTemplate` 등 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ef8e781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 저는 테디입니다. AI 어시스턴트입니다. 무엇을 도와드릴까요? 😊'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다.\"),\n",
    "        (\"human\", \"반가워요!\"),\n",
    "        (\"ai\", \"안녕하세요! 무엇을 도와드릴까요?\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = chat_template | model\n",
    "chain.invoke({'name':'Teddy', 'user_input': '당신의 이름은 무엇입니까?'}).content.split('</think>\\n\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6bb2f",
   "metadata": {},
   "source": [
    "### MessagePlaceHolder\n",
    "\n",
    "```python\n",
    "MessagesPlaceholder(\n",
    "    variable_name: str\n",
    ")\n",
    "```\n",
    "\n",
    "MessagePlaceHolder는 특정 값을 넣을 때 사용된다. 보통 **이전 대화 기록**이나 **메모리 출력** 등을 넣는 데 사용된다. input_variables와 차이가 없어보이나 아래와 같은 차이가 있다.\n",
    "\n",
    "#### input_variables vs. MessagePlaceHolder\n",
    "| 항목 | 설명 |\n",
    "|------|------|\n",
    "| `input_variables` | `{}` 안에 들어갈 일반 문자열 값 (예: 숫자, 이름 등) |\n",
    "| `MessagesPlaceholder` | 여러 개의 메시지 객체를 프롬프트 중간에 삽입할 때 사용 |\n",
    "\n",
    "- `input_variables`는 문자열 템플릿을 채우는 용도\n",
    "- `MessagesPlaceholder`는 Chat 구조 내에서 역할을 갖는 메시지들을 중간에 삽입할 수 있도록 함\n",
    "- 보통 **이전 대화 기록**이나 **메모리 출력** 등을 넣는 데 사용됨\n",
    "\n",
    "\n",
    "실제 MessagePlaceholder가 치환될 수 있는 값들은 아래와 같다. Input으로 들어가게 될때는 (role, message) 형태로 들어가게 된다.\n",
    "\n",
    "| 클래스           | 역할 설명         | role 값       | 예시 |\n",
    "|------------------|------------------|---------------|------|\n",
    "| `HumanMessage`   | 사용자 입력 역할   | `\"user\"`       | `HumanMessage(content=\"안녕\")` |\n",
    "| `AIMessage`      | AI 응답 역할      | `\"assistant\"`  | `AIMessage(content=\"안녕하세요!\")` |\n",
    "| `SystemMessage`  | 시스템 지침 역할   | `\"system\"`     | `SystemMessage(content=\"너는 요약 AI야\")` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23178124",
   "metadata": {},
   "source": [
    "LLM의 Ouput을 다음 LLM의 Input으로 사용하고 싶을 때, PlaceHolder를 사용하는 법은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a224836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "사용자의 질문은 \"오늘 날씨 어때?\"입니다. 이 질문은 날씨 정보를 제공하는 것이 필요합니다. 하지만 현재 시점에서 실제 날씨 데이터를 가져오는 기능이 없기 때문에, 대답을 할 수 없습니다. 대신, 사용자에게 예측 가능한 날씨 정보를 제공하거나, 날씨를 확인할 수 있는 방법을 제안할 수 있습니다. 예를 들어, \"지역을 알려주시면 해당 지역의 날씨를 확인해 드릴 수 있어요.\"라고 말할 수 있습니다. 또한, 사용자에게 날씨를 확인하는 방법을 안내해줄 수도 있습니다. 예를 들어, \"앱이나 웹사이트를 사용하여 실시간 날씨를 확인할 수 있어요.\"라고 말할 수 있습니다. 이와 같은 대답은 사용자를 도움을 주고, 사용자가 원하는 정보를 제공할 수 있도록 도와줍니다. 또한, 사용자에게 친절하게 대답하는 것이 중요합니다. 사용자가 질문을 하면, 즉시 대답하고, 필요하다면 추가적인 정보를 제공하는 것이 좋습니다. 따라서, 사용자에게 지역을 묻고, 날씨를 확인하는 방법을 제안하는 것이 적절합니다.\n",
      "</think>\n",
      "\n",
      "안녕하세요! 날씨 정보를 알려드릴 수 없어요. 현재 시점에서 실시간 날씨 데이터를 확인할 수 없기 때문입니다. 하지만 지역을 알려주시면, 해당 지역의 날씨를 확인해 드릴 수 있어요. 예를 들어, \"서울\", \"부산\", \"경기\" 등 어떤 지역이신가요? 또는 날씨를 확인할 수 있는 앱이나 웹사이트를 추천해드릴 수도 있어요! 😊\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 첫 번째 프롬프트 (단일 질문)\n",
    "first_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 친절한 AI야.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# 사용자 질문\n",
    "user_question = \"오늘 날씨 어때?\"\n",
    "\n",
    "# LLM에 첫 번째 질문 수행\n",
    "first_result = model.invoke(first_prompt.format_messages(question=user_question))\n",
    "print(first_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e090040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리의 대화는 사용자가 오늘 날씨를 묻고, AI가 실시간 날씨 정보를 제공할 수 없음을 설명하며 지역을 지정하거나 앱 추천을 제안하는 내용을 포함한 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 생성된 응답을 기반으로 대화 기록 구성\n",
    "first_chat_result = first_result.content.split('</think>\\n\\n')[1]\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=user_question),\n",
    "    AIMessage(content=first_chat_result)\n",
    "]\n",
    "\n",
    "# Step 3: 두 번째 프롬프트 - 이전 대화를 문맥으로 사용\n",
    "second_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 요약 전문 AI야.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),  # 이전 메시지 들어갈 자리\n",
    "    (\"human\", \"방금 대화를 한 문장으로 요약해줘.\"),\n",
    "])\n",
    "\n",
    "# history에 이전 대화 기록 전달\n",
    "second_result = model.invoke(\n",
    "    second_prompt.format_messages(\n",
    "        history=chat_history\n",
    "    )\n",
    ")\n",
    "\n",
    "print(second_result.content.split('</think>\\n\\n')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4689c9",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "\n",
    "```python\n",
    "FewShotPromptTemplate(\n",
    "    examples: List[dict],\n",
    "    example_prompt: PromptTemplate,\n",
    "    prefix: Optional[str] = None,\n",
    "    suffix: Optional[str] = None,\n",
    "    input_variables: List[str],\n",
    "    example_separator: str = \"\\n\\n\"\n",
    ")\n",
    "```\n",
    "\n",
    "| 인자명                 | 타입               | 설명                       |\n",
    "| ------------------- | ---------------- | ------------------------ |\n",
    "| `examples`          | `List[dict]`     | 예시 샘플 데이터 (입출력 쌍)        |\n",
    "| `example_prompt`    | `PromptTemplate` | 각 예시를 어떻게 표현할지 정의        |\n",
    "| `prefix`            | `str`            | 예시 앞에 붙는 도입 문구           |\n",
    "| `suffix`            | `str`            | 실제 사용자 입력이 들어가는 부분       |\n",
    "| `input_variables`   | `List[str]`      | `suffix`에서 사용되는 변수 목록    |\n",
    "| `example_separator` | `str`            | 예시 사이의 구분자 (기본값: `\\n\\n`) |\n",
    "\n",
    "\n",
    "FewShot이란 LLM에 실제 예시를 주어짐으로서, Output의 형태를 지정하거나 혹은 어떠한 형태를 원하는지 정보를 전달하여 할루시네이션을 줄일 수 있습니다. 실제 사용하는 예시는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ccaafef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Dataset Check\n",
      "Q: apple\n",
      "A: 사과\n",
      "\n",
      "\n",
      "FewShot Prompt 결과 확인.\n",
      "다음은 영어 단어를 한국어로 번역한 예시입니다:\n",
      "\n",
      "\n",
      "Q: apple\n",
      "A: 사과\n",
      "\n",
      "Q: car\n",
      "A: 자동차\n",
      "\n",
      "Q: school\n",
      "A: 학교\n",
      "\n",
      "Q: computer\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "# Fewshow에 사용할 Examples를 선언합니다. 해당 값은 List[dict]입니다.\n",
    "examples = [\n",
    "    {\"english\": \"apple\", \"korean\": \"사과\"},\n",
    "    {\"english\": \"car\", \"korean\": \"자동차\"},\n",
    "    {\"english\": \"school\", \"korean\": \"학교\"}\n",
    "]\n",
    "\n",
    "# 각 예시를 LLM에 넣을 Prompt Template입니다.\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"english\", \"korean\"], # 변수로 받을 값 입니다. english, korean을 입력으로 받습니다.\n",
    "    template=\"Q: {english}\\nA: {korean}\" # 입력받은 값으로서 어떻게 출력할지 형태를 정합니다.\n",
    ")\n",
    "\n",
    "print('Single Dataset Check')\n",
    "print(example_prompt.format(**examples[0]))\n",
    "\n",
    "# FewShotTemplate 선언\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"다음은 영어 단어를 한국어로 번역한 예시입니다:\\n\",\n",
    "    suffix=\"Q: {english}\\nA:\",\n",
    "    input_variables=[\"english\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# 실제 입력 넣기\n",
    "print(\"\\n\\nFewShot Prompt 결과 확인.\")\n",
    "formatted_prompt = few_shot_prompt.format(english=\"computer\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95ec821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: computer  \n",
      "A: 컴퓨터\n"
     ]
    }
   ],
   "source": [
    "# LLM에 사용할 Template 만들기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 영어 단어를 한국어로 번역해주는 AI입니다.\"),\n",
    "    (\"human\", formatted_prompt)\n",
    "])\n",
    "\n",
    "print(model.invoke(chat_prompt.format_messages()).content.split('</think>\\n\\n')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9c438",
   "metadata": {},
   "source": [
    "### Example Selector\n",
    "\n",
    "- 참조 사이트: https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/\n",
    "\n",
    "Fewshot의 예시를 많이 넣어두고, RAG에서 선택해서 필요한 Examples를 가져다 쓰게 하는게 보통 방법이다.  \n",
    "다양한 방법이 있지만, 몇몇 방법만 예제로서 확인해보자.\n",
    "\n",
    "\n",
    "#### Step1. Dataset 생성.\n",
    "\n",
    "실제 Example Selector에서 사용할 Dataset을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccaaf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"english\": \"apple\", \"korean\": \"사과\"},\n",
    "    {\"english\": \"car\", \"korean\": \"자동차\"},\n",
    "    {\"english\": \"school\", \"korean\": \"학교\"},\n",
    "    {\"english\": \"computer\", \"korean\": \"컴퓨터\"},\n",
    "    {\"english\": \"house\", \"korean\": \"집\"},\n",
    "    {\"english\": \"pencil\", \"korean\": \"연필\"},\n",
    "    {\"english\": \"teacher\", \"korean\": \"선생님\"},\n",
    "    {\"english\": \"student\", \"korean\": \"학생\"},\n",
    "    {\"english\": \"city\", \"korean\": \"도시\"},\n",
    "    {\"english\": \"book\", \"korean\": \"책\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878b259",
   "metadata": {},
   "source": [
    "**Appendix: Text to Embedding**: 실제 Embedding Model을 사용하는 것이 가장 정확하나. Local환경이 3090 GPU 1개로는 qwen3-4b + embedding model을 같이 올릴 수 없다. 따라서, dummy로 embedding하는 function으로서 embedding 값을 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1923264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101.0, 110.0, 103.0, 108.0, 105.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Dummy Embedding Function\n",
    "1. text의 앞부분 5글자만 가져와서 float으로 변경\n",
    "2. embed_documents, embed_query는 Embeddings에서 정의된 함수로서 내부적으로 싸져서 자동으로 사용할 수 있게 한다.\n",
    "'''\n",
    "\n",
    "class DummyEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # 실제로는 Qwen3 임베딩 사용 필요\n",
    "        return [[float(ord(c)) for c in text][:5] for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return [float(ord(c)) for c in text][:5]\n",
    "    \n",
    "embedding = DummyEmbeddings()\n",
    "embedding.embed_documents(texts=['english'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a338d96",
   "metadata": {},
   "source": [
    "**추가 팁: 어떤값이 Embedding 되는 것 인가?**\n",
    "\n",
    "DummyEmbedding을 보게 되면, document를 embedding하고, query를 embedding하는 것을 알 수 있다. 어떤 값을 기준으로 Embedding하는지 알아보면 다음과 같다.\n",
    "\n",
    "1. input_keys가 지정되었으면 그 key만 추출\n",
    "2. 지정되지 않았으면 dict.keys()[0] (첫 번째 key) 사용\n",
    "3. 해당 문자열들을 .embed_documents()로 전달\n",
    "4. 나중에 .select_examples(input) 호출 시, embed_query(input)과 벡터 유사도를 비교하여 가장 가까운 예시 k개 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d938e791",
   "metadata": {},
   "source": [
    "#### Step2. Example Selector 종류\n",
    "\n",
    "실제로 많이 사용되는 ExampleSelector를 살펴보자.\n",
    "\n",
    "| Selector 클래스                          | 설명                                     |\n",
    "| ------------------------------------- | -------------------------------------- |\n",
    "| `SemanticSimilarityExampleSelector`   | 입력값과 임베딩 유사도가 높은 예시 `k`개 선택 (가장 많이 사용) |\n",
    "| `LengthBasedExampleSelector`          | 예시 길이 기반으로 `max_length` 이내에서 가능한 만큼 선택 |\n",
    "| `MaxMarginalRelevanceExampleSelector` | 유사성과 다양성을 동시에 고려하여 예시 선택               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a44ac2",
   "metadata": {},
   "source": [
    "#### Step3. SemanticSimilarityExampleSelector\n",
    "\n",
    "실제, ExampleSelector중 하나로서, 해당 결과를 확인해보자.\n",
    "\n",
    "```python\n",
    "SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples: List[dict],\n",
    "    embedding: Embeddings,\n",
    "    vectorstore_cls: Type[VectorStore],\n",
    "    k: int = 4,\n",
    "    input_keys: Optional[List[str]] = None\n",
    ")\n",
    "```\n",
    "| 인자명               | 타입                | 설명                                               |\n",
    "| ----------------- | ----------------- | ------------------------------------------------ |\n",
    "| `examples`        | `List[dict]`      | 예시 입력/출력 쌍들 (예: {\"english\": ..., \"korean\": ...}) |\n",
    "| `embedding`       | `Embeddings` 객체   | 예시의 입력을 벡터로 변환할 임베딩 모델                           |\n",
    "| `vectorstore_cls` | `VectorStore` 클래스 | 벡터를 저장하고 유사도 검색할 벡터스토어 클래스 (예: `FAISS`)          |\n",
    "| `k`               | `int`             | 입력값과 유사한 예시 몇 개를 선택할지 결정                         |\n",
    "| `input_keys`      | `List[str]` (선택)  | 어떤 key의 값을 임베딩 대상으로 사용할지 지정 (기본은 첫 키)            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce13eb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음은 영어 단어를 한국어로 번역한 예시입니다:\n",
      "\n",
      "\n",
      "Q: pencil\n",
      "A: 연필\n",
      "\n",
      "Q: student\n",
      "A: 학생\n",
      "\n",
      "Q: keyboard\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "\n",
    "# Prompt Template으로서 english, korean을 Few Shot으로 넣기 위해 지정.\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"english\", \"korean\"], # key값의 2개는 english, korean이다.\n",
    "    template=\"Q: {english}\\nA: {korean}\" # 해당 값을 활용한 template는 다음과 같다.\n",
    ")\n",
    "\n",
    "# SemanticSearch로서 Embedding Vector를 구성한다.\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples, # input dataset\n",
    "    embeddings=DummyEmbeddings(),  # 현재 GPU Memory 부족으로서 Dummy Embedding Function을 사용하여 Embedding 한다.\n",
    "    vectorstore_cls=FAISS, # 저장하는 vector store는 FAISS로서 지정한다.\n",
    "    k=2 # Few Shot의 갯수를 2개로 지정한다.\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"다음은 영어 단어를 한국어로 번역한 예시입니다:\\n\",\n",
    "    suffix=\"Q: {english}\\nA:\",\n",
    "    input_variables=[\"english\"]\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format(english=\"keyboard\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa708a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: keyboard  \n",
      "A: 키보드\n"
     ]
    }
   ],
   "source": [
    "# LLM에 사용할 Template 만들기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 영어 단어를 한국어로 번역해주는 AI입니다.\"),\n",
    "    (\"human\", few_shot_prompt.format(english=\"keyboard\"))\n",
    "])\n",
    "\n",
    "print(model.invoke(chat_prompt.format_messages()).content.split('</think>\\n\\n')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84602036",
   "metadata": {},
   "source": [
    "**Appendix. multi_index for RAG**: 아래와 같이 여러 key값을 모두 embedding에 사용할 수 있다.\n",
    "\n",
    "현재, DummyEmbedding에서는 2개 이상의 key값으로 Embedding하는 function을 지정하지 않아서, 사용하지는 못한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8566881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSearch로서 Embedding Vector를 구성한다.\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples, # input dataset\n",
    "    embeddings=DummyEmbeddings(),  # 현재 GPU Memory 부족으로서 Dummy Embedding Function을 사용하여 Embedding 한다.\n",
    "    vectorstore_cls=FAISS, # 저장하는 vector store는 FAISS로서 지정한다.\n",
    "    k=2, # Few Shot의 갯수를 2개로 지정한다.\n",
    "    input_keys=[\"english\", \"korean\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c712e",
   "metadata": {},
   "source": [
    "### LangChain-Hub\n",
    "\n",
    "LangChain-Hub에 들어가게 되면, 아래 Figure와 같이, 남들이 사용한 Prompt를 확인하여 사용할 수 있다.\n",
    "- link: https://smith.langchain.com/hub\n",
    "\n",
    "![png](./img/1.png)\n",
    "\n",
    "Prompt는 LangCahin-Hub에 연결하여, push하거나 pull하여 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c789c82",
   "metadata": {},
   "source": [
    "<hr>\n",
    "참조: <a href=\"https://wikidocs.net/book/14314\">LangChain 위키독스</a><br>\n",
    "참조: <a href=\"https://github.com/wjddyd66/LLM/tree/main\">원본 코드</a><br>\n",
    "\n",
    "코드에 문제가 있거나 궁금한 점이 있으면 wjddyd66@naver.com으로  Mail을 남겨주세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
